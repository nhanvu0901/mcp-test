{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "\n",
    "\n",
    "def query(query_text: str, limit: int = 5) -> list[str]:\n",
    "    \"\"\"\n",
    "    Query the Qdrant vector database with a text query and return matching results.\n",
    "    \n",
    "    Args:\n",
    "        query_text (str): The text query to search for\n",
    "        limit (int): Maximum number of results to return (default: 5)\n",
    "    \n",
    "    Returns:\n",
    "        list[str]: List of matching text results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Generate embedding for the query text\n",
    "        query_embedding = embedding_model.embed_query(query_text)\n",
    "        \n",
    "        # Search in Qdrant\n",
    "        search_results = qdrant_client.search(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            query_vector=query_embedding,\n",
    "            limit=limit\n",
    "        )\n",
    "        \n",
    "        # Extract text content from results\n",
    "        results = []\n",
    "        for result in search_results:\n",
    "            # Assuming the text content is stored in payload under 'text' key\n",
    "            # Adjust the key name based on your actual data structure\n",
    "            if 'text' in result.payload:\n",
    "                results.append(result.payload['text'])\n",
    "            elif 'content' in result.payload:\n",
    "                results.append(result.payload['content'])\n",
    "            else:\n",
    "                # If no text field found, convert payload to string\n",
    "                results.append(str(result.payload))\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during query: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_AsyncGeneratorContextManager' object has no attribute 'get_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m rag_client = client.session(\u001b[33m\"\u001b[39m\u001b[33mRAGService\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m document_client = client.session(\u001b[33m\"\u001b[39m\u001b[33mDocumentService\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m rag_tools = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[43mrag_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tools\u001b[49m()\n\u001b[32m     42\u001b[39m document_tools = \u001b[38;5;28;01mawait\u001b[39;00m document_client.get_tools()\n\u001b[32m     44\u001b[39m prompt = \u001b[33m\"\u001b[39m\u001b[33mYou are a RAG agent, please query the RAGService for the information if the user asks\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAttributeError\u001b[39m: '_AsyncGeneratorContextManager' object has no attribute 'get_tools'"
     ]
    }
   ],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "import time\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "AZURE_OPENAI_API_KEY=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_MODEL_NAME=os.getenv(\"AZURE_OPENAI_MODEL_NAME\")\n",
    "AZURE_OPENAI_MODEL_API_VERSION=os.getenv(\"AZURE_OPENAI_MODEL_API_VERSION\")\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "    model=AZURE_OPENAI_MODEL_NAME,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=AZURE_OPENAI_MODEL_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    temperature=0,\n",
    "    max_tokens=5000\n",
    ")\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"RAGService\": {\n",
    "            \"url\": \"http://localhost:8002/sse\",\n",
    "            \"transport\": \"sse\",\n",
    "        },\n",
    "        \"DocumentService\": {\n",
    "            \"url\": \"http://localhost:8001/sse\",\n",
    "            \"transport\": \"sse\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()\n",
    "prompt = \"You are a RAG agent, please query the RAGService for the information if the user asks\"\n",
    "agent = create_react_agent(model, tools, prompt=prompt)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "answer = await astream_graph(\n",
    "    agent, {\"messages\": \"What's MCP? Answer with out query\"}\n",
    ")\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ðŸ”„ Node: \u001b[1;36magent\u001b[0m ðŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "==================================================\n",
      "ðŸ”„ Node: \u001b[1;36mtools\u001b[0m ðŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "The **Message Control Protocol (MCP)** is a lightweight and flexible communication framework designed to facilitate structured and reliable interactions between distributed components, particularly in microservices architectures and AI agent ecosystems. MCP abstracts away low-level networking complexities and provides a consistent interface for message passing, command execution, and event propagation across services or modules. At its core, MCP supports a publish-subscribe model, enabling agents to react to events and state changes in real time, while also supporting request-response patterns for synchronous communication. One of MCP's key strengths lies in its extensibilityâ€”developers can define custom message schemas, integrate serialization formats like JSON or Protobuf, and enforce message validation with minimal overhead. Itâ€™s commonly used in agent orchestration layers, such as LangChainâ€™s multi-agent systems, where coordination and context-passing between reasoning agents is\n",
      "message validation with minimal overhead. Itâ€™s commonly used in agent orchestration layers, such as LangChainâ€™s multi-agent systems, where coordination and context-passing between reasoning agents is critical. In AI applications, MCP allows agents to delegate tasks, share intermediate outputs, or access centralized tools such as databases, vector stores, or external APIs. Additionally, MCP implementations typically support features like message history tracking, priority queuing, retry mechanisms, and timeouts, which are essential for building robust, production-grade systems. Whether implemented in Python or TypeScript, MCP servers can act as coordination hubs, while adapters (clients) encapsulate logic for publishing commands and subscribing to topics. This modular and decoupled design makes MCP particularly suited for dynamic environments where services may be scaled independently, evolve over time, or require fine-grained control over messaging flows.\n",
      "Component                                          Description                           Library\n",
      "0         MCP Server   Core component managing services and event routing  @langchain/mcp-adapters, fastmcp\n",
      "1    Service Adapter           Connects external services to MCP protocol           @langchain/mcp-adapters\n",
      "2      Event Handler            Processes messages and triggers workflows             fastmcp, custom logic\n",
      "3         Client API  Allows frontend/backend apps to send/receive events           fastmcp, WebSocket/HTTP\n",
      "4  Persistence Layer               Stores state or logs from interactions            fastmcp, Redis/MongoDB\n",
      "5          Scheduler             Triggers services at scheduled intervals                 fastmcp.scheduler\n",
      "6    Monitoring Tool               Observes MCP traffic and system health       custom, Prometheus, Grafana\n",
      "7    Security Module               Manages authentication and permissions    custom, OAuth2/JWT integration\n",
      "Component                                          Description                           Library\n",
      "0         MCP Server   Core component managing services and event routing  @langchain/mcp-adapters, fastmcp\n",
      "1    Service Adapter           Connects external services to MCP protocol           @langchain/mcp-adapters\n",
      "2      Event Handler            Processes messages and triggers workflows             fastmcp, custom logic\n",
      "3         Client API  Allows frontend/backend apps to send/receive events           fastmcp, WebSocket/HTTP\n",
      "4  Persistence Layer               Stores state or logs from interactions            fastmcp, Redis/MongoDB\n",
      "5          Scheduler             Triggers services at scheduled intervals                 fastmcp.scheduler\n",
      "6    Monitoring Tool               Observes MCP traffic and system health       custom, Prometheus, Grafana\n",
      "7    Security Module               Manages authentication and permissions    custom, OAuth2/JWT integration\n",
      "The MCP protocol defines three core primitives that servers can implement:  \n",
      "| Primitive | Control               | Description                                         | Example Use                  |\n",
      "|-----------|-----------------------|-----------------------------------------------------|------------------------------|\n",
      "| Prompts   | User-controlled       | Interactive templates invoked by user choice        | Slash commands, menu options |\n",
      "| Resources | Application-controlled| Contextual data managed by the client application   | File contents, API responses |\n",
      "| Tools     | Model-controlled      | Functions exposed to the LLM to take actions        | API calls, data updates      |\n",
      "==================================================\n",
      "ðŸ”„ Node: \u001b[1;36magent\u001b[0m ðŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "MCP stands for **Message Control Protocol**. It is a lightweight and flexible communication framework designed to enable structured and reliable interactions between distributed components, especially in microservices architectures and AI agent ecosystems.\n",
      "\n",
      "### Key Features of MCP:\n",
      "1. **Communication Patterns**:\n",
      "   - Supports both **publish-subscribe** models for real-time event handling and **request-response** patterns for synchronous communication.\n",
      "   \n",
      "2. **Extensibility**:\n",
      "   - Developers can define custom message schemas, integrate serialization formats (e.g., JSON, Protobuf), and enforce message validation with minimal overhead.\n",
      "\n",
      "3. **Use Cases**:\n",
      "   - Commonly used in agent orchestration layers, such as LangChainâ€™s multi-agent systems, where coordination and context-passing between reasoning agents is critical.\n",
      "   - Facilitates task delegation, intermediate output sharing, and access to centralized tools like databases, vector stores, or external APIs.\n",
      "\n",
      "4. **Robustness**:\n",
      "   - Features like message history tracking, priority queuing, retry mechanisms, and timeouts make it suitable for production-grade systems.\n",
      "\n",
      "5. **Modular Design**:\n",
      "   - MCP servers act as coordination hubs, while adapters (clients) handle publishing commands and subscribing to topics. This design supports dynamic environments where services can scale independently or evolve over time.\n",
      "\n",
      "### Core Components:\n",
      "| Component            | Description                                           | Example Libraries/Tools         |\n",
      "|-----------------------|-------------------------------------------------------|----------------------------------|\n",
      "| MCP Server           | Manages services and event routing                    | `@langchain/mcp-adapters`, `fastmcp` |\n",
      "| Service Adapter      | Connects external services to the MCP protocol         | `@langchain/mcp-adapters`       |\n",
      "| Event Handler        | Processes messages and triggers workflows              | `fastmcp`, custom logic         |\n",
      "| Client API           | Allows apps to send/receive events                     | `fastmcp`, WebSocket/HTTP       |\n",
      "| Persistence Layer    | Stores state or logs from interactions                 | `fastmcp`, Redis/MongoDB        |\n",
      "| Scheduler            | Triggers services at scheduled intervals               | `fastmcp.scheduler`             |\n",
      "| Monitoring Tool      | Observes MCP traffic and system health                 | Prometheus, Grafana             |\n",
      "| Security Module      | Manages authentication and permissions                 | OAuth2/JWT integration          |\n",
      "\n",
      "### Core Primitives:\n",
      "| Primitive  | Control                | Description                                      | Example Use                  |\n",
      "|------------|------------------------|--------------------------------------------------|------------------------------|\n",
      "| **Prompts** | User-controlled        | Interactive templates invoked by user choice     | Slash commands, menu options |\n",
      "| **Resources** | Application-controlled | Contextual data managed by the client application | File contents, API responses |\n",
      "| **Tools**    | Model-controlled      | Functions exposed to the LLM to take actions     | API calls, data updates      |\n",
      "\n",
      "MCP is particularly useful in dynamic and distributed systems, enabling seamless communication and coordination between components."
     ]
    }
   ],
   "source": [
    "print(f\"Time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing file: mcp.md\n",
      "Collection 'mcp' does not exist. Creating it.\n",
      "Extracting text from mcp.md...\n",
      "Text extracted (first 200 chars): # Meeting Minutes\n",
      "\n",
      "## June 19, 2025\n",
      "\n",
      "### Attendees\n",
      "- Alice\n",
      "- Bob\n",
      "- Charlie\n",
      "\n",
      "### Discussion Points\n",
      "1.  **Project Alpha**: Reviewed progress. On track for phase 1 completion.\n",
      "2.  **Budget Review**: Disc...\n",
      "Processing and adding chunks to Qdrant using 'markdown_header' method...\n",
      "Successfully upserted 3 chunks to Qdrant for document ID: mcp.md\n",
      "Document processing complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_mcp_adapters.client import MCPClient\n",
    "from langgraph_supervisor import create_supervisor\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Azure OpenAI Configuration\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_MODEL_NAME = os.getenv(\"AZURE_OPENAI_MODEL_NAME\")\n",
    "AZURE_OPENAI_MODEL_API_VERSION = os.getenv(\"AZURE_OPENAI_MODEL_API_VERSION\")\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "    model=AZURE_OPENAI_MODEL_NAME,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=AZURE_OPENAI_MODEL_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    temperature=0,\n",
    "    max_tokens=5000\n",
    ")\n",
    "\n",
    "async def setup_agents():\n",
    "    \"\"\"Setup individual MCP clients and agents\"\"\"\n",
    "    \n",
    "    # RAG Service MCP Client\n",
    "    rag_client = MCPClient(\n",
    "        url=\"http://localhost:8002/sse\",\n",
    "        transport=\"sse\"\n",
    "    )\n",
    "    \n",
    "    # Document Service MCP Client  \n",
    "    document_client = MCPClient(\n",
    "        url=\"http://localhost:8001/sse\",\n",
    "        transport=\"sse\"\n",
    "    )\n",
    "    \n",
    "    # Get tools from each service\n",
    "    rag_tools = await rag_client.get_tools()\n",
    "    document_tools = await document_client.get_tools()\n",
    "    \n",
    "    # Create RAG Agent\n",
    "    rag_agent = create_react_agent(\n",
    "        model=model,\n",
    "        tools=rag_tools,\n",
    "        name=\"rag_expert\",\n",
    "        prompt=(\n",
    "            \"You are a RAG (Retrieval-Augmented Generation) expert. \"\n",
    "            \"Use vector search to find relevant information from the knowledge base. \"\n",
    "            \"Provide accurate, contextual answers based on retrieved documents. \"\n",
    "            \"Always cite your sources when possible.\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Create Document Agent\n",
    "    document_agent = create_react_agent(\n",
    "        model=model,\n",
    "        tools=document_tools,\n",
    "        name=\"document_expert\", \n",
    "        prompt=(\n",
    "            \"You are a document processing expert. \"\n",
    "            \"Handle document operations like reading, parsing, and extracting information. \"\n",
    "            \"Process various file formats and provide structured summaries. \"\n",
    "            \"Focus on accuracy and completeness in document analysis.\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return rag_agent, document_agent\n",
    "\n",
    "async def create_supervisor_workflow():\n",
    "    \"\"\"Create the supervisor workflow with specialized agents\"\"\"\n",
    "    \n",
    "    # Setup agents\n",
    "    rag_agent, document_agent = await setup_agents()\n",
    "    \n",
    "    # Create supervisor workflow\n",
    "    workflow = create_supervisor(\n",
    "        agents=[rag_agent, document_agent],\n",
    "        model=model,\n",
    "        prompt=(\n",
    "            \"You are a team supervisor managing specialized AI agents. \"\n",
    "            \"Route tasks based on their nature:\\n\"\n",
    "            \"- For knowledge retrieval, semantic search, or answering questions from existing data: use rag_expert\\n\"\n",
    "            \"- For document processing, file analysis, or content extraction: use document_expert\\n\"\n",
    "            \"Always choose the most appropriate agent for the task.\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return workflow\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    \n",
    "    # Create supervisor workflow\n",
    "    workflow = await create_supervisor_workflow()\n",
    "    \n",
    "    # Compile the workflow\n",
    "    app = workflow.compile()\n",
    "    \n",
    "    # Example usage\n",
    "    result = await app.ainvoke({\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": \"What's MCP? Please query the RAGService for information.\"\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    print(\"=== Supervisor Response ===\")\n",
    "    for message in result[\"messages\"]:\n",
    "        print(f\"{message['role']}: {message['content']}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Alternative synchronous wrapper for easier usage\n",
    "def run_supervisor_query(query: str):\n",
    "    \"\"\"Synchronous wrapper for running supervisor queries\"\"\"\n",
    "    \n",
    "    async def _run():\n",
    "        workflow = await create_supervisor_workflow()\n",
    "        app = workflow.compile()\n",
    "        \n",
    "        result = await app.ainvoke({\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
    "        })\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    return asyncio.run(_run())\n",
    "\n",
    "# Example usage functions\n",
    "async def example_rag_query():\n",
    "    \"\"\"Example RAG query\"\"\"\n",
    "    workflow = await create_supervisor_workflow()\n",
    "    app = workflow.compile()\n",
    "    \n",
    "    result = await app.ainvoke({\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Search for information about machine learning algorithms in the knowledge base.\"\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    return result\n",
    "\n",
    "async def example_document_query():\n",
    "    \"\"\"Example document processing query\"\"\"\n",
    "    workflow = await create_supervisor_workflow()\n",
    "    app = workflow.compile()\n",
    "    \n",
    "    result = await app.ainvoke({\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": \"Please analyze and summarize the contents of the uploaded PDF document.\"\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the main example\n",
    "    asyncio.run(main())\n",
    "    \n",
    "    # Or use the synchronous wrapper\n",
    "    # result = run_supervisor_query(\"What's MCP? Please search the knowledge base.\")\n",
    "    # print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Supervisor Response ===\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'HumanMessage' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 189\u001b[39m\n\u001b[32m    183\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    188\u001b[39m     \u001b[38;5;66;03m# This only runs when executed as a script, not in Jupyter\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 144\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== Supervisor Response ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m result[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmessage\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage[\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    145\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: 'HumanMessage' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from mcp import ClientSession\n",
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langgraph_supervisor import create_supervisor\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Azure OpenAI Configuration\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_MODEL_NAME = os.getenv(\"AZURE_OPENAI_MODEL_NAME\")\n",
    "AZURE_OPENAI_MODEL_API_VERSION = os.getenv(\"AZURE_OPENAI_MODEL_API_VERSION\")\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "    model=AZURE_OPENAI_MODEL_NAME,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=AZURE_OPENAI_MODEL_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    temperature=0,\n",
    "    max_tokens=5000\n",
    ")\n",
    "\n",
    "async def setup_rag_agent():\n",
    "    \"\"\"Setup RAG agent with single MCP client\"\"\"\n",
    "    \n",
    "    # Connect to RAG Service using streamablehttp_client\n",
    "    async with streamablehttp_client(\"http://localhost:8002/mcp/\") as (read, write, _):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            # Initialize the connection\n",
    "            await session.initialize()\n",
    "            \n",
    "            # Get tools from RAG service\n",
    "            rag_tools = await load_mcp_tools(session)\n",
    "            \n",
    "            # Create RAG Agent\n",
    "            rag_agent = create_react_agent(\n",
    "                model=model,\n",
    "                tools=rag_tools,\n",
    "                name=\"rag_expert\",\n",
    "                prompt=(\n",
    "                    \"You are a RAG (Retrieval-Augmented Generation) expert. \"\n",
    "                    \"Use vector search to find relevant information from the knowledge base. \"\n",
    "                    \"Provide accurate, contextual answers based on retrieved documents. \"\n",
    "                    \"Always cite your sources when possible.\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            return rag_agent, session  # Return session to keep connection alive\n",
    "\n",
    "async def create_supervisor_workflow():\n",
    "    \"\"\"Create the supervisor workflow with RAG agent only\"\"\"\n",
    "    \n",
    "    # Setup RAG agent\n",
    "    rag_agent, session = await setup_rag_agent()\n",
    "    \n",
    "    # Create supervisor workflow\n",
    "    workflow = create_supervisor(\n",
    "        agents=[rag_agent],\n",
    "        model=model,\n",
    "        prompt=(\n",
    "            \"You are a team supervisor managing a RAG expert. \"\n",
    "            \"Route knowledge retrieval, semantic search, or question answering tasks to rag_expert. \"\n",
    "            \"Always use the rag_expert for information retrieval from the knowledge base.\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return workflow, session\n",
    "\n",
    "# Alternative approach using MultiServerMCPClient for single server\n",
    "async def setup_rag_agent_alternative():\n",
    "    \"\"\"Alternative setup using MultiServerMCPClient with single server\"\"\"\n",
    "    from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "    \n",
    "    # Single server configuration\n",
    "    client = MultiServerMCPClient({\n",
    "        \"rag_service\": {\n",
    "            \"url\": \"http://localhost:8002/sse\",\n",
    "            \"transport\": \"sse\",\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    # Get tools\n",
    "    rag_tools = await client.get_tools()\n",
    "    \n",
    "    # Create RAG Agent\n",
    "    rag_agent = create_react_agent(\n",
    "        model=model,\n",
    "        tools=rag_tools,\n",
    "        name=\"rag_expert\",\n",
    "        prompt=(\n",
    "            \"You are a RAG expert. \"\n",
    "            \"Search the knowledge base to answer questions. \"\n",
    "            \"Provide accurate answers with source citations.\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return rag_agent\n",
    "\n",
    "async def create_supervisor_workflow_alternative():\n",
    "    \"\"\"Alternative supervisor workflow\"\"\"\n",
    "    \n",
    "    # Setup RAG agent\n",
    "    rag_agent = await setup_rag_agent_alternative()\n",
    "    \n",
    "    # Create supervisor workflow\n",
    "    workflow = create_supervisor(\n",
    "        agents=[rag_agent],\n",
    "        model=model,\n",
    "        prompt=(\n",
    "            \"You manage a RAG expert. \"\n",
    "            \"For any knowledge questions or information retrieval: use rag_expert.\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return workflow\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main execution function - using alternative approach\"\"\"\n",
    "    \n",
    "    # Create supervisor workflow (using simpler MultiServerMCPClient approach)\n",
    "    workflow = await create_supervisor_workflow_alternative()\n",
    "    \n",
    "    # Compile the workflow\n",
    "    app = workflow.compile()\n",
    "    \n",
    "    # Example usage\n",
    "    result = await app.ainvoke({\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": \"What's MCP? Please query the RAGService for information.\"\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    print(\"=== Supervisor Response ===\")\n",
    "    for message in result[\"messages\"]:\n",
    "        print(f\"{message['role']}: {message['content']}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Synchronous wrapper\n",
    "def run_supervisor_query(query: str):\n",
    "    \"\"\"Synchronous wrapper for running supervisor queries\"\"\"\n",
    "    \n",
    "    async def _run():\n",
    "        workflow = await create_supervisor_workflow_alternative()\n",
    "        app = workflow.compile()\n",
    "        \n",
    "        result = await app.ainvoke({\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
    "        })\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    return asyncio.run(_run())\n",
    "\n",
    "# Test functions\n",
    "async def test_rag_query():\n",
    "    \"\"\"Test RAG query\"\"\"\n",
    "    workflow = await create_supervisor_workflow_alternative()\n",
    "    app = workflow.compile()\n",
    "    \n",
    "    result = await app.ainvoke({\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Please tell me what is Model Context Protocol?\"\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    print(\"=== RAG Test Response ===\")\n",
    "    for message in result[\"messages\"]:\n",
    "        print(f\"{message.role}: {message.content}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # This only runs when executed as a script, not in Jupyter\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from mcp import ClientSession\n",
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langgraph_supervisor import create_supervisor\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Azure OpenAI Configuration\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_MODEL_NAME = os.getenv(\"AZURE_OPENAI_MODEL_NAME\")\n",
    "AZURE_OPENAI_MODEL_API_VERSION = os.getenv(\"AZURE_OPENAI_MODEL_API_VERSION\")\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "    model=AZURE_OPENAI_MODEL_NAME,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=AZURE_OPENAI_MODEL_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    temperature=0,\n",
    "    max_tokens=5000\n",
    ")\n",
    "\n",
    "def log_time(message, start=None):\n",
    "    now = datetime.now()\n",
    "    if start:\n",
    "        duration = now - start\n",
    "        print(f\"[{now}] {message} - Duration: {duration}\")\n",
    "    else:\n",
    "        print(f\"[{now}] {message}\")\n",
    "    return now\n",
    "\n",
    "async def setup_rag_agent_alternative():\n",
    "    from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "    \n",
    "    log_time(\"Initializing MultiServerMCPClient\")\n",
    "    start = datetime.now()\n",
    "\n",
    "    client = MultiServerMCPClient({\n",
    "        \"rag_service\": {\n",
    "            \"url\": \"http://localhost:8002/sse\",\n",
    "            \"transport\": \"sse\",\n",
    "        }\n",
    "    })\n",
    "\n",
    "    log_time(\"Fetching RAG tools from MCP\", start)\n",
    "    start = datetime.now()\n",
    "    rag_tools = await client.get_tools()\n",
    "    log_time(\"Fetched RAG tools\", start)\n",
    "\n",
    "    log_time(\"Creating RAG Agent\")\n",
    "    start = datetime.now()\n",
    "    rag_agent = create_react_agent(\n",
    "        model=model,\n",
    "        tools=rag_tools,\n",
    "        name=\"rag_expert\",\n",
    "        prompt=(\n",
    "            \"You are a RAG expert. \"\n",
    "            \"Search the knowledge base to answer questions. \"\n",
    "            \"Provide accurate answers with source citations.\"\n",
    "        )\n",
    "    )\n",
    "    log_time(\"Created RAG Agent\", start)\n",
    "    return rag_agent\n",
    "\n",
    "async def create_supervisor_workflow_alternative():\n",
    "    log_time(\"Setting up RAG Agent (Alternative)\")\n",
    "    start = datetime.now()\n",
    "    rag_agent = await setup_rag_agent_alternative()\n",
    "    log_time(\"RAG Agent setup complete\", start)\n",
    "\n",
    "    log_time(\"Creating Supervisor Workflow\")\n",
    "    start = datetime.now()\n",
    "    workflow = create_supervisor(\n",
    "        agents=[rag_agent],\n",
    "        model=model,\n",
    "        prompt=(\n",
    "            \"You manage a RAG expert. \"\n",
    "            \"For any knowledge questions or information retrieval: use rag_expert.\"\n",
    "        )\n",
    "    )\n",
    "    log_time(\"Created Supervisor Workflow\", start)\n",
    "    return workflow\n",
    "\n",
    "async def main():\n",
    "    log_time(\"Main execution started\")\n",
    "    start_total = datetime.now()\n",
    "\n",
    "    workflow = await create_supervisor_workflow_alternative()\n",
    "    \n",
    "    log_time(\"Compiling workflow\")\n",
    "    start = datetime.now()\n",
    "    app = workflow.compile()\n",
    "    log_time(\"Workflow compiled\", start)\n",
    "\n",
    "    log_time(\"Sending user query to app\")\n",
    "    start = datetime.now()\n",
    "    result = await app.ainvoke({\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": \"What's MCP? Please query the RAGService for information.\"\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    "    log_time(\"Query completed\", start)\n",
    "\n",
    "    print(\"\\n=== Supervisor Response ===\")\n",
    "    for message in result[\"messages\"]:\n",
    "        print(f\"{message['role']}: {message['content']}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    log_time(\"Main execution finished\", start_total)\n",
    "\n",
    "def run_supervisor_query(query: str):\n",
    "    async def _run():\n",
    "        workflow = await create_supervisor_workflow_alternative()\n",
    "        app = workflow.compile()\n",
    "        \n",
    "        start = datetime.now()\n",
    "        result = await app.ainvoke({\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
    "        })\n",
    "        log_time(\"Synchronous query completed\", start)\n",
    "\n",
    "        return result\n",
    "\n",
    "    return asyncio.run(_run())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"web_sk.csv\"\n",
    "\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sk = df[df['Web'].str.contains('.sk')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep website that contain .sk in the url Web\n",
    "\n",
    "df_sk.to_csv(\"web_sk_categorized.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
